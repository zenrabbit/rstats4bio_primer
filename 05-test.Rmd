# Test {#test}
![](./test.png){width=3in}   

Put your practice to the test. Here are some excellent [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) to consider for biostats in R, and this is a useful read on [good enough practices in scientific computing](https://arxiv.org/abs/1609.00037) [@RN5002]. The goal here was not to become data scientists nor biostatisticians but to encourage you to consider developing and refining your critical thinking skills in the context of evidence, data, and statistical reasoning.  

#### Learning outcomes {-}  
1. Complete fundamental exploratory data analysis on a representative dataset culminating with a fair and reasonable statistical model.   
2. Interpret a statistical analyses that you completed with a focus on relevance, significance, and logic.   
3. Communicate biostatistical work clearly and effectively to others.      

#### Critical thinking {-}  
At times in many disciplines of biological research, we need to be open to experimentation that is fair, transparent, and replicable but that is implemented based on available data. This experimentatation can also happen after we have data. It can be an exercise in fitting the most appropriate or parsimonous models [@RN1873], applying experimental design principles [@RN6381], and of course invoking critical thinking. This is not to say we are going on fishing expeditions, but that that at times, we have only certain data to describe a system and are tasked or obligated to use the best possible evidence we have to infer relevant processes. For instance, we might compile field data, data from online resources or data products for climate or landscapes, or reuse data on traits on genetics and link these different evidence streams together to explore a question. Critical thinking in statistics can be an important framework that we leverage to not only do the statistics and fit models but also ensure that we are able to ask the questions we need to. In summary, we have data and need an answer but have to use open and transparent thinking with statistics to find the best question.  

### Workflow for hackathons  {-}  
A hackathon in data science and the computational science is a fixed-duration, collaborative endeavor to develop a solution for a focussed challenge. The goal is to have a reasonably functional first-approximation that is viable and/or describes the key processes for a system or dataset. It is a blend of hacking and marathon to race or sprint towards a clear endpoint in development. In the data and statistical sciences, we intensively work to deepen our understanding of evidence ideally with key data visualizations and a model that predicts or describes key outcomes. The advantage of setting a reasonably short but fair duration is that it reduces the likelihood that tangents are unduly developed. It also hones your coding, research skills, and statistical reasoning through practiced mental model application of statistics to new data to tell a balanced and reproducible, transparent story.     

1. Get the data.  

2. Read the metadata (and if you get stuck, look up from online resources or related/similar datasets the potential meaning of opaque variable names). Nomenclature and annotation shorthand terminology in a field can be highly specific at times.  

3. Consider and ensure that you understand the individual vectors or variables (inspect the dataframe).  

4. Develop an informal or formal [data map](https://www.integrate.io/blog/data-mapping-an-overview-of-data-mapping-and-its-technology/) - picture a [Sankey diagram](https://www.data-to-viz.com/graph/sankey.html) (conceptual semantic visualization of relationships between variables).    

5. Dig into online resources or literature to ideate on important questions, novel gaps, key theories, or even basic fundamental science that supports these data.  

6. Decide on focus and key purpose and begin to plan out an analytical workflow.  

7. Determine if you have sufficient data, i.e., consider if you need to augment these data. Augmenting data can be from novel data sources or from reclassification of existing data.  

8. Begin your exploration of the dimensions and scope of the variables you were interested in using ([skimr](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html), min, max, [fitdistrplus](https://cran.r-project.org/web/packages/fitdistrplus/index.html), or str-like functions or tools in R).  

9. Now, adopt the r4ds workflow such as [Fig 1.1](https://r4ds.had.co.nz/introduction.html), and use plots such as histograms or boxplots to understand depth and range of data, use basic tests as the t.test to explore differences, and prepare for your final statistical model and keystone plot to show the differences you tested.  

10. Code and test your main model to address the overarching goal. Decide and revise the best/most representative instance of data viz that illuminates the salient process or patterns examined. 

If you favor this method of collaborative work in your lab or team, here are [ten simple rules to run a successful BioHackathon](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007808).  


### Test adventure time {-}  
York University, [Keele Campus](https://en.wikipedia.org/wiki/Keele_Campus_(York_University)) is a small urban forest mixed with grasslands and open space. The master gardeners measured nearly 7000 trees over the course of two years. These data were recently compiled and [published](https://knb.ecoinformatics.org/view/doi%3A10.5063%2FQ81BGH). There are many fascinating and compelling questions to explore that can support evidence-informed decisions and valuation estimates for this place ecologically, environmentally, and from a trait or species-level perspective. This challenge as a summative test is thus relatively more open ended. Given these data, collected and now published, what can we do to enhance our biological and social understanding and appreciation for a university campus that support people, other animals, and plants. Explore the data, define a relevant challenge or set of questions that would benefit the stakeholders or local community or inform our understanding of a biological theory, and demonstrate your mastery of critical thinking in statistics.  Submit your work to [turnitin.com](https://www.turnitin.com) as PDF including the code, annotation, rationale, interpretation, and outputs from the viz, EDA, and model(s) that supported your thinking.  

### Metadata for test data {-}    
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
meta <- read_csv("./metadata.csv")
knitr::kable(meta, booktabs = TRUE)
```

### Test data {-}    
```{r include=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
trees <- read_csv(url("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A1e738f4d-f491-4b40-b55a-e8395c5349ce"))  
trees

```

### Clean code {-}  
Effective coding so that others can read it and understand it - not just machines - is an art and a science. Object and function naming that is intuitive really helps. Functions to streamline repeated operations, and annotation to explain steps with headers are all useful. This approach to literate coding for humans is sometimes entitled 'clean code'. [**Here**](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13961) is a short paper with some tips and tricks relevant to your work when you need to share it [@RNclean].  

### Rubric {-}  

Remember, we are working together to hone our statistical reasoning skills.  

**The goal is to tell a story with these data**. 

It does not need to be super complex, but it does need to showcase your skills in understanding key principles such as a GLM with appropriate data visualizations - but any reasonable test that MATCHES the story you tell is great.

Show your work of exploring the data in plots and basic stats, develop your idea, test it, and then have a final key plot showing the relationship you tested.  


```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
test <- read_csv("./key_test.csv")
knitr::kable(test, booktabs = TRUE)

```
